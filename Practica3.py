# -*- coding: utf-8 -*-
"""Práctica3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dmSBz1DGKMKcrs1oPRZ0TNEeB_7Rpb2y

# Imports
"""

import numpy as np
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from keras.datasets import mnist
from keras.utils import to_categorical
from sklearn.metrics import classification_report, confusion_matrix #Apartado 4

"""# Primera parte

## Apartado 1.
El problema que estamos a punto de abordar se basa en la clasificación de imágenes utilizando distintos tipos de redes (Neuronales Artificiales y Convolucionales) valorando críticamente ambos modelos y sus resultados. En este caso las imágenes a clasificar serán dígitos escritos a mano.

## Apartado 2.

###Código
"""

# Cargar y preprocesar el conjunto de datos MNIST
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Normalizar las imágenes y convertirlas a vectores unidimensionales
x_train = x_train.astype('float32') / 255
x_test = x_test.astype('float32') / 255
x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))
x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))

# Codificar las etiquetas en one-hot
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)

# Definir el modelo MLP
model = Sequential()
model.add(Dense(512, activation='relu', input_shape=(784,)))
model.add(Dense(10, activation='softmax'))

# Compilar el modelo
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Entrenar el modelo
model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))

# Evaluar el modelo en el conjunto de prueba
loss, accuracy = model.evaluate(x_test, y_test)
print(f'Loss: {loss}, Accuracy: {accuracy}')

"""###Crítica de la Red Neuronal Alimentada Hacia Adelante (MLP) para clasificación de imágenes:

Primero preprocesamos los datos normalizando todas las imagenes a 255 pixeles, las matrices de 28x28 se aplanan para poder introducirlas en los inputs de la red alimentada hacia adelante (784 inputs en total), despues las etiquetas se codifican en one-hot para adaptarse a la salida de la red(0,0,0,0,1,0,0,0,0) donde se marcan con 1 la clase a la que pertenece un ejemplo y con 0 a las que no pertenece. Creamos un modelo de red alimentada hacia adelante con un numero de 512 neuronas en la capa oculta y con una funcion de activacion ReLU para poder ajustar el modelo a funciones no lineales. Por ultimo hemos añadido una capa softax con 10 unidades en la salida, puesto que se trata de un modelo de clasificacion de números escritos a mano. Para ajustar la tasa de aprendizaje y el momento de este modelo usamos ADAM, utilizando como funcion de pérdida la entropía cruzada y monitorizamos el rendimiento del modelo con la métrica "accuracy" o precisión. El entrenamiento se divide en 10 epocas de 32 ejemplos cada uno, se eligen 10 para evitar un sobreajuste del modelo. Elegimos un tamaño de lote de 32 ejemplos ya que un tamaño pequeño de lote puede contibuir a la estabilidad del modelo en el aspecto de que puede evitar minimos locales y puede ayudar a la generalizacion del modelo ya que cada actualizacion se basa en un conjunto mas variado de datos. Aunque podiamos haber fijado un numero menor de lote para aprovechar sus beneficios sobre el modelo, hemos decidido fijarlo en 32 contemplando el numero de ejemplos que nos aporta mnist y la velocidad de convergencia del modelo, ya que un numero mayor de lote favorece en tiempo a la convergencia puesto que se realizan actualizaciones de pesos menos frecuentes.

Parte 1 y 3 de la critica juntas. La MLP al no ser un modelo convolutivo, no realiza de matrices con filtros, herramienta que se usa mucho para reconocer patrones, formas, texturas y superficies visuales. Aumenta el riesgo de sobreajuste cuando la resolucion de las imagenes es alta, ya que al aumentar la resolucion se aumenta el numero de entradas en la red y por tanto se realiza un mayor numero de conexiones entre neuronas con sus correspondientes pesos que son los parametros de este modelo. Ademas si el conjunto de datos es pequeño, este riesgo aumenta aun mas, ya que el modelo puede no generalizar adecuadamente y aprender unicamente las caracteristicas de estos pocos ejemplos que tiene. Reduccion de la dimensionalidad no controlada, en muchas ocasiones las capas ocultas pueden generar representaciones de una alta dimensionalidad sin un control sobre la interpretacion de las dimensiones. Por este mismo motivo, por las capas ocultas, podemos perder el control sobre las caracteristicas que se estan aprendiendo pueden estar aprendiendose caracteristicas irrelevantes o muy importantes sin tener nosotros una perspectiva clara de lo que esta sucediendo dentro de la red. Una posible solucion que podriamos tomar es el ajuste del numero de neuronas y capas a implementar, pero al no existir una formula para averiguarlo, este proceso se complicaria mucho.

## Apartado 3.

###Código
"""

#---------Primera_Parte---------Apartado3-------------
# Cargar y preprocesar el conjunto de datos MNIST
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Agregar una dimensión para los canales (escala de grises)
x_train = np.expand_dims(x_train, axis=-1)
x_test = np.expand_dims(x_test, axis=-1)

# Normalizar las imágenes y codificar las etiquetas en one-hot
x_train = x_train.astype('float32') / 255
x_test = x_test.astype('float32') / 255
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)

# Definir el modelo CNN
model = Sequential()
model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(Flatten())
model.add(Dense(64, activation='relu'))
model.add(Dense(10, activation='softmax'))

# Compilar el modelo
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Entrenar el modelo
model.fit(x_train, y_train, epochs=10, batch_size=64, validation_data=(x_test, y_test))

# Evaluar el modelo en el conjunto de prueba
loss, accuracy = model.evaluate(x_test, y_test)
print(f'Loss: {loss}, Accuracy: {accuracy}')



"""###Crítica de la Red Convolucional para clasificación de imagenes:

> Añadir blockquote


En la evaluación crítica de la Red Convolucional para la clasificación de imágenes, se destacan varias razones fundamentales que respaldan su eficacia. En primer lugar, la incorporación de capas convolucionales y de pooling en la red convolucional (CNN) proporciona invariancia espacial, permitiendo la captura de patrones locales invariantes ante cambios de posición. Esto se traduce en una mejora significativa en el reconocimiento de objetos, independientemente de su ubicación en la imagen. Además, la CNN demuestra ser más eficiente en términos de la cantidad de parámetros requeridos, lo que reduce el riesgo de sobreajuste, especialmente en conjuntos de datos limitados, mejorando así la generalización del modelo. La utilización de capas convolucionales también facilita la detección automática de características locales, mejorando la capacidad del modelo para identificar patrones visuales cruciales. Por último, la eficiencia en la extracción de características es impulsada por la capacidad de la CNN para aprender automáticamente representaciones más relevantes de las imágenes, eliminando la necesidad de realizar ingeniería manual de características y, en consecuencia, mejorando significativamente el rendimiento al capturar automáticamente las características más importantes de las imágenes.

##Apartado 4.

###Código
"""

# Evaluar la capacidad de entrenamiento
train_loss, train_acc = model.evaluate(x_train, y_train)
print(f"Training Accuracy: {train_acc*100:.2f}%")

# Evaluar la capacidad de prueba
test_loss, test_acc = model.evaluate(x_test, y_test)
print(f"Testing Accuracy: {test_acc*100:.2f}%")

# Obtener predicciones en el conjunto de prueba
y_pred = np.argmax(model.predict(x_test), axis=1)
y_true = np.argmax(y_test, axis=1)

# Imprimir métricas de clasificación
print("Confusion Matrix:\n", confusion_matrix(y_true, y_pred))
print("\nClassification Report:\n", classification_report(y_true, y_pred))

"""###Evaluación

##Apartado 5.

###Mejoras

-Aumento de Datos (Data Augmentation): El aumento de datos implica aplicar
transformaciones aleatorias a las imagenes de entrenamiento, como rotaciones,
traslaciones y reflejos horizontales. Esto ayuda a que el modelo se vuelva mas
robusto y generalice mejor a nuevas imagenes. Por lo que, a mas cantidad de
conexiones, mas aprendera.

-Dropout: Ya que el Dropout es una tecnica de regularizacion que consiste en apagar
aleatoriamente algunas neuronas durante el entrenamiento. Esto ayudaría a
prevenir el sobreajuste, ya que obliga a la red a aprender representaciones mas
robustas al no depender excesivamente de neuronas específicas.

-Cambio de Optimizador: Adam es un optimizador popular, pero se podría
experimentar con otros como RMSprop.
Regularizacion: La regularizacion, en este caso L2, ayuda a prevenir el sobreajuste
penalizando los pesos grandes. Esto puede ser util cuando se tienen capas
convolucionales con muchos parametros. Ademas, se podría utlizar para reducir su
complejidad al reducir sus los costes y por ende realiza una reduce la probabilidad
de sobreajuste.

-Uso de Redes Siamesas y el Triplet Loss: Las redes siamesas son muy utilizadas
para procesar datos similares y diferentes. Ademas el uso de la funcion Triplet Loss
ayuda a minimizar la distancia entre similares y maximizar entre diferentes.
"""